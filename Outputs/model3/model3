2025-02-25 21:49:07,805 - INFO - ======== Starting Model Training ========
2025-02-25 21:49:07,805 - INFO - Loading dataset...
2025-02-25 21:49:08,311 - INFO - Dataset loaded successfully!
2025-02-25 21:49:08,311 - INFO - Initializing the model...
2025-02-25 21:49:08,394 - INFO - Model initialized with 291,013 trainable parameters.
2025-02-25 21:49:08,394 - INFO - Model Architecture: 
MobileNetV2ForCIFAR8M(
  (mobilenet_v2): MobileNetV2(
    (features): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (classifier): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=512, out_features=512, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.9, inplace=False)
      (4): Linear(in_features=512, out_features=5, bias=True)
    )
  )
)
2025-02-25 21:49:08,394 - INFO - Starting training for 80 epochs...
2025-02-25 21:49:08,979 - INFO - Tr_Loss: 1.6314, val_loss: 1.5361, Tr_acc: 22.571428571428573, val_ac: 37.86666666666667
2025-02-25 21:49:09,186 - INFO - Tr_Loss: 1.5458, val_loss: 1.4731, Tr_acc: 30.17142857142857, val_ac: 48.4
2025-02-25 21:49:09,416 - INFO - Tr_Loss: 1.5049, val_loss: 1.4171, Tr_acc: 34.4, val_ac: 57.46666666666667
2025-02-25 21:49:09,639 - INFO - Tr_Loss: 1.4311, val_loss: 1.3625, Tr_acc: 40.91428571428571, val_ac: 63.333333333333336
2025-02-25 21:49:09,851 - INFO - Tr_Loss: 1.3847, val_loss: 1.3100, Tr_acc: 43.2, val_ac: 66.66666666666667
2025-02-25 21:49:10,076 - INFO - Tr_Loss: 1.3319, val_loss: 1.2578, Tr_acc: 49.65714285714286, val_ac: 68.0
2025-02-25 21:49:10,298 - INFO - Tr_Loss: 1.2670, val_loss: 1.2073, Tr_acc: 51.885714285714286, val_ac: 70.0
2025-02-25 21:49:10,513 - INFO - Tr_Loss: 1.2334, val_loss: 1.1577, Tr_acc: 56.17142857142857, val_ac: 70.93333333333334
2025-02-25 21:49:10,737 - INFO - Tr_Loss: 1.1733, val_loss: 1.1091, Tr_acc: 58.51428571428571, val_ac: 72.26666666666667
2025-02-25 21:49:10,955 - INFO - Tr_Loss: 1.1337, val_loss: 1.0627, Tr_acc: 60.74285714285714, val_ac: 72.66666666666667
2025-02-25 21:49:11,157 - INFO - Tr_Loss: 1.0808, val_loss: 1.0199, Tr_acc: 63.42857142857143, val_ac: 73.6
2025-02-25 21:49:11,349 - INFO - Tr_Loss: 1.0500, val_loss: 0.9799, Tr_acc: 62.628571428571426, val_ac: 74.26666666666667
2025-02-25 21:49:11,561 - INFO - Tr_Loss: 1.0165, val_loss: 0.9427, Tr_acc: 65.71428571428571, val_ac: 74.53333333333333
2025-02-25 21:49:11,782 - INFO - Tr_Loss: 0.9621, val_loss: 0.9084, Tr_acc: 67.14285714285714, val_ac: 75.06666666666666
2025-02-25 21:49:11,997 - INFO - Tr_Loss: 0.9351, val_loss: 0.8764, Tr_acc: 67.42857142857143, val_ac: 75.73333333333333
2025-02-25 21:49:12,223 - INFO - Tr_Loss: 0.9372, val_loss: 0.8467, Tr_acc: 68.85714285714286, val_ac: 75.73333333333333
2025-02-25 21:49:12,440 - INFO - Tr_Loss: 0.8906, val_loss: 0.8192, Tr_acc: 70.17142857142858, val_ac: 75.86666666666666
2025-02-25 21:49:12,667 - INFO - Tr_Loss: 0.8732, val_loss: 0.7942, Tr_acc: 70.62857142857143, val_ac: 76.13333333333334
2025-02-25 21:49:12,866 - INFO - Tr_Loss: 0.8317, val_loss: 0.7716, Tr_acc: 72.0, val_ac: 76.8
2025-02-25 21:49:13,091 - INFO - Tr_Loss: 0.8093, val_loss: 0.7501, Tr_acc: 72.28571428571429, val_ac: 77.86666666666666
2025-02-25 21:49:13,285 - INFO - Tr_Loss: 0.7970, val_loss: 0.7300, Tr_acc: 72.74285714285715, val_ac: 78.53333333333333
2025-02-25 21:49:13,494 - INFO - Tr_Loss: 0.7590, val_loss: 0.7107, Tr_acc: 75.71428571428571, val_ac: 78.8
2025-02-25 21:49:13,722 - INFO - Tr_Loss: 0.7540, val_loss: 0.6932, Tr_acc: 75.31428571428572, val_ac: 79.33333333333333
2025-02-25 21:49:13,969 - INFO - Tr_Loss: 0.7368, val_loss: 0.6774, Tr_acc: 74.68571428571428, val_ac: 79.73333333333333
2025-02-25 21:49:14,195 - INFO - Tr_Loss: 0.7064, val_loss: 0.6623, Tr_acc: 77.2, val_ac: 79.86666666666666
2025-02-25 21:49:14,415 - INFO - Tr_Loss: 0.6993, val_loss: 0.6482, Tr_acc: 75.37142857142857, val_ac: 79.6
2025-02-25 21:49:14,633 - INFO - Tr_Loss: 0.6783, val_loss: 0.6354, Tr_acc: 76.8, val_ac: 80.13333333333334
2025-02-25 21:49:14,844 - INFO - Tr_Loss: 0.6587, val_loss: 0.6228, Tr_acc: 78.51428571428572, val_ac: 80.53333333333333
2025-02-25 21:49:15,065 - INFO - Tr_Loss: 0.6501, val_loss: 0.6106, Tr_acc: 78.45714285714286, val_ac: 80.66666666666667
2025-02-25 21:49:15,281 - INFO - Tr_Loss: 0.6435, val_loss: 0.5994, Tr_acc: 77.54285714285714, val_ac: 80.53333333333333
2025-02-25 21:49:15,478 - INFO - Tr_Loss: 0.6262, val_loss: 0.5888, Tr_acc: 79.31428571428572, val_ac: 80.93333333333334
2025-02-25 21:49:15,674 - INFO - Tr_Loss: 0.6208, val_loss: 0.5793, Tr_acc: 78.45714285714286, val_ac: 80.66666666666667
2025-02-25 21:49:15,890 - INFO - Tr_Loss: 0.6013, val_loss: 0.5716, Tr_acc: 79.77142857142857, val_ac: 80.8
2025-02-25 21:49:16,090 - INFO - Tr_Loss: 0.5909, val_loss: 0.5635, Tr_acc: 79.65714285714286, val_ac: 80.8
2025-02-25 21:49:16,278 - INFO - Tr_Loss: 0.5751, val_loss: 0.5559, Tr_acc: 80.05714285714286, val_ac: 80.93333333333334
2025-02-25 21:49:16,464 - INFO - Tr_Loss: 0.5643, val_loss: 0.5482, Tr_acc: 81.31428571428572, val_ac: 81.33333333333333
2025-02-25 21:49:16,672 - INFO - Tr_Loss: 0.5664, val_loss: 0.5420, Tr_acc: 81.31428571428572, val_ac: 81.2
2025-02-25 21:49:16,872 - INFO - Tr_Loss: 0.5399, val_loss: 0.5357, Tr_acc: 82.4, val_ac: 81.2
2025-02-25 21:49:17,091 - INFO - Tr_Loss: 0.5323, val_loss: 0.5290, Tr_acc: 81.71428571428571, val_ac: 81.33333333333333
2025-02-25 21:49:17,274 - INFO - Tr_Loss: 0.5337, val_loss: 0.5239, Tr_acc: 80.91428571428571, val_ac: 81.6
2025-02-25 21:49:17,467 - INFO - Tr_Loss: 0.5299, val_loss: 0.5189, Tr_acc: 82.34285714285714, val_ac: 81.86666666666666
2025-02-25 21:49:17,658 - INFO - Tr_Loss: 0.5155, val_loss: 0.5126, Tr_acc: 82.51428571428572, val_ac: 82.0
2025-02-25 21:49:17,849 - INFO - Tr_Loss: 0.5028, val_loss: 0.5072, Tr_acc: 82.45714285714286, val_ac: 82.4
2025-02-25 21:49:18,039 - INFO - Tr_Loss: 0.4946, val_loss: 0.5013, Tr_acc: 83.88571428571429, val_ac: 82.26666666666667
2025-02-25 21:49:18,232 - INFO - Tr_Loss: 0.5076, val_loss: 0.4971, Tr_acc: 82.8, val_ac: 82.4
2025-02-25 21:49:18,419 - INFO - Tr_Loss: 0.4830, val_loss: 0.4927, Tr_acc: 83.02857142857142, val_ac: 82.26666666666667
2025-02-25 21:49:18,620 - INFO - Tr_Loss: 0.4728, val_loss: 0.4885, Tr_acc: 84.05714285714286, val_ac: 82.13333333333334
2025-02-25 21:49:18,800 - INFO - Tr_Loss: 0.4441, val_loss: 0.4838, Tr_acc: 85.37142857142857, val_ac: 82.26666666666667
2025-02-25 21:49:18,997 - INFO - Tr_Loss: 0.4504, val_loss: 0.4794, Tr_acc: 85.65714285714286, val_ac: 82.66666666666667
2025-02-25 21:49:19,181 - INFO - Tr_Loss: 0.4505, val_loss: 0.4759, Tr_acc: 85.14285714285714, val_ac: 82.4
2025-02-25 21:49:19,377 - INFO - Tr_Loss: 0.4345, val_loss: 0.4716, Tr_acc: 86.28571428571429, val_ac: 82.66666666666667
2025-02-25 21:49:19,560 - INFO - Tr_Loss: 0.4194, val_loss: 0.4670, Tr_acc: 86.4, val_ac: 82.93333333333334
2025-02-25 21:49:19,755 - INFO - Tr_Loss: 0.4230, val_loss: 0.4638, Tr_acc: 84.97142857142858, val_ac: 82.93333333333334
2025-02-25 21:49:19,943 - INFO - Tr_Loss: 0.4303, val_loss: 0.4610, Tr_acc: 85.54285714285714, val_ac: 82.8
2025-02-25 21:49:20,136 - INFO - Tr_Loss: 0.4158, val_loss: 0.4591, Tr_acc: 86.28571428571429, val_ac: 83.33333333333333
2025-02-25 21:49:20,320 - INFO - Tr_Loss: 0.4216, val_loss: 0.4562, Tr_acc: 85.48571428571428, val_ac: 83.2
2025-02-25 21:49:20,512 - INFO - Tr_Loss: 0.4040, val_loss: 0.4530, Tr_acc: 87.88571428571429, val_ac: 83.06666666666666
2025-02-25 21:49:20,702 - INFO - Tr_Loss: 0.4040, val_loss: 0.4503, Tr_acc: 86.57142857142857, val_ac: 83.6
2025-02-25 21:49:20,886 - INFO - Tr_Loss: 0.3835, val_loss: 0.4467, Tr_acc: 87.37142857142857, val_ac: 83.73333333333333
2025-02-25 21:49:21,076 - INFO - Tr_Loss: 0.3821, val_loss: 0.4427, Tr_acc: 87.82857142857142, val_ac: 84.0
2025-02-25 21:49:21,258 - INFO - Tr_Loss: 0.3817, val_loss: 0.4409, Tr_acc: 86.51428571428572, val_ac: 84.13333333333334
2025-02-25 21:49:21,451 - INFO - Tr_Loss: 0.3786, val_loss: 0.4383, Tr_acc: 87.02857142857142, val_ac: 84.13333333333334
2025-02-25 21:49:21,644 - INFO - Tr_Loss: 0.3628, val_loss: 0.4367, Tr_acc: 88.17142857142858, val_ac: 84.13333333333334
2025-02-25 21:49:21,826 - INFO - Tr_Loss: 0.3487, val_loss: 0.4355, Tr_acc: 89.25714285714285, val_ac: 84.0
2025-02-25 21:49:22,020 - INFO - Tr_Loss: 0.3355, val_loss: 0.4331, Tr_acc: 87.71428571428571, val_ac: 84.0
2025-02-25 21:49:22,202 - INFO - Tr_Loss: 0.3444, val_loss: 0.4313, Tr_acc: 88.91428571428571, val_ac: 84.13333333333334
2025-02-25 21:49:22,399 - INFO - Tr_Loss: 0.3290, val_loss: 0.4281, Tr_acc: 88.4, val_ac: 84.4
2025-02-25 21:49:22,590 - INFO - Tr_Loss: 0.3257, val_loss: 0.4259, Tr_acc: 89.31428571428572, val_ac: 84.26666666666667
2025-02-25 21:49:22,804 - INFO - Tr_Loss: 0.3311, val_loss: 0.4239, Tr_acc: 88.68571428571428, val_ac: 84.26666666666667
2025-02-25 21:49:22,990 - INFO - Tr_Loss: 0.3181, val_loss: 0.4234, Tr_acc: 89.37142857142857, val_ac: 84.26666666666667
2025-02-25 21:49:23,175 - INFO - Tr_Loss: 0.3101, val_loss: 0.4219, Tr_acc: 90.22857142857143, val_ac: 84.26666666666667
2025-02-25 21:49:23,364 - INFO - Tr_Loss: 0.3137, val_loss: 0.4203, Tr_acc: 89.77142857142857, val_ac: 84.26666666666667
2025-02-25 21:49:23,554 - INFO - Tr_Loss: 0.3151, val_loss: 0.4180, Tr_acc: 89.54285714285714, val_ac: 84.13333333333334
2025-02-25 21:49:23,743 - INFO - Tr_Loss: 0.3188, val_loss: 0.4158, Tr_acc: 90.17142857142858, val_ac: 84.26666666666667
2025-02-25 21:49:23,940 - INFO - Tr_Loss: 0.2892, val_loss: 0.4133, Tr_acc: 90.51428571428572, val_ac: 84.53333333333333
2025-02-25 21:49:24,135 - INFO - Tr_Loss: 0.2757, val_loss: 0.4121, Tr_acc: 90.45714285714286, val_ac: 85.06666666666666
2025-02-25 21:49:24,328 - INFO - Tr_Loss: 0.2999, val_loss: 0.4107, Tr_acc: 90.05714285714286, val_ac: 84.93333333333334
2025-02-25 21:49:24,516 - INFO - Tr_Loss: 0.2750, val_loss: 0.4096, Tr_acc: 92.0, val_ac: 84.93333333333334
2025-02-25 21:49:24,702 - INFO - Tr_Loss: 0.2661, val_loss: 0.4074, Tr_acc: 91.48571428571428, val_ac: 85.06666666666666
2025-02-25 21:49:24,891 - INFO - Tr_Loss: 0.2651, val_loss: 0.4059, Tr_acc: 92.28571428571429, val_ac: 84.93333333333334
2025-02-25 21:49:24,893 - INFO - Saving trained model and training results...
2025-02-25 21:49:25,305 - INFO - Starting model evaluation...
2025-02-25 21:49:25,333 - INFO - Test Loss: 0.4007
2025-02-25 21:49:25,334 - INFO - Test Accuracy: 85.00%
2025-02-25 21:49:25,334 - INFO - ======== Model Training Completed! ========
