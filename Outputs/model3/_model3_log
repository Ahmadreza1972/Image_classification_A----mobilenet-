2025-03-08 17:12:06,749 - INFO - ======== Starting Model Training ========
2025-03-08 17:12:06,755 - INFO - Loading dataset...
2025-03-08 17:12:07,241 - INFO - Dataset loaded successfully!
2025-03-08 17:12:07,241 - INFO - Initializing the model...
2025-03-08 17:12:07,313 - INFO - Model initialized with 453,317 trainable parameters
2025-03-08 17:12:07,313 - INFO - Model Architecture: 
MobileNetV2ForCIFAR8M(
  (mobilenet_v2): MobileNetV2(
    (features): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
          )
          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (classifier): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=512, out_features=512, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.7, inplace=False)
      (4): Linear(in_features=512, out_features=256, bias=True)
      (5): ReLU()
      (6): Dropout(p=0.7, inplace=False)
      (7): Linear(in_features=256, out_features=128, bias=True)
      (8): ReLU()
      (9): Dropout(p=0.7, inplace=False)
      (10): Linear(in_features=128, out_features=5, bias=True)
    )
  )
)
2025-03-08 17:12:07,327 - INFO - Starting training for 70 epochs...
2025-03-08 17:12:07,948 - INFO - Tr_Loss: 1.6166, val_loss: 1.6078, Tr_acc: 19.885714285714286, val_ac: 24.266666666666666
2025-03-08 17:12:08,204 - INFO - Tr_Loss: 1.6116, val_loss: 1.6045, Tr_acc: 21.02857142857143, val_ac: 30.933333333333334
2025-03-08 17:12:08,421 - INFO - Tr_Loss: 1.6063, val_loss: 1.6006, Tr_acc: 23.942857142857143, val_ac: 36.666666666666664
2025-03-08 17:12:08,636 - INFO - Tr_Loss: 1.6022, val_loss: 1.5960, Tr_acc: 24.914285714285715, val_ac: 43.06666666666667
2025-03-08 17:12:08,866 - INFO - Tr_Loss: 1.5985, val_loss: 1.5900, Tr_acc: 24.514285714285716, val_ac: 50.53333333333333
2025-03-08 17:12:09,073 - INFO - Tr_Loss: 1.5942, val_loss: 1.5818, Tr_acc: 28.34285714285714, val_ac: 54.8
2025-03-08 17:12:09,285 - INFO - Tr_Loss: 1.5841, val_loss: 1.5711, Tr_acc: 34.114285714285714, val_ac: 57.06666666666667
2025-03-08 17:12:09,508 - INFO - Tr_Loss: 1.5704, val_loss: 1.5558, Tr_acc: 37.142857142857146, val_ac: 57.06666666666667
2025-03-08 17:12:09,745 - INFO - Tr_Loss: 1.5593, val_loss: 1.5387, Tr_acc: 41.82857142857143, val_ac: 59.333333333333336
2025-03-08 17:12:09,960 - INFO - Tr_Loss: 1.5474, val_loss: 1.5202, Tr_acc: 44.74285714285714, val_ac: 61.333333333333336
2025-03-08 17:12:10,177 - INFO - Tr_Loss: 1.5331, val_loss: 1.5025, Tr_acc: 48.22857142857143, val_ac: 62.666666666666664
2025-03-08 17:12:10,395 - INFO - Tr_Loss: 1.5203, val_loss: 1.4831, Tr_acc: 53.142857142857146, val_ac: 64.26666666666667
2025-03-08 17:12:10,603 - INFO - Tr_Loss: 1.5042, val_loss: 1.4686, Tr_acc: 56.0, val_ac: 65.86666666666666
2025-03-08 17:12:10,826 - INFO - Tr_Loss: 1.4967, val_loss: 1.4549, Tr_acc: 57.885714285714286, val_ac: 66.26666666666667
2025-03-08 17:12:11,078 - INFO - Tr_Loss: 1.4876, val_loss: 1.4428, Tr_acc: 61.31428571428572, val_ac: 67.73333333333333
2025-03-08 17:12:11,294 - INFO - Tr_Loss: 1.4791, val_loss: 1.4343, Tr_acc: 63.42857142857143, val_ac: 68.8
2025-03-08 17:12:11,532 - INFO - Tr_Loss: 1.4677, val_loss: 1.4257, Tr_acc: 64.28571428571429, val_ac: 71.06666666666666
2025-03-08 17:12:11,758 - INFO - Tr_Loss: 1.4599, val_loss: 1.4184, Tr_acc: 66.8, val_ac: 72.13333333333334
2025-03-08 17:12:12,013 - INFO - Tr_Loss: 1.4526, val_loss: 1.4145, Tr_acc: 68.34285714285714, val_ac: 72.93333333333334
2025-03-08 17:12:12,233 - INFO - Tr_Loss: 1.4474, val_loss: 1.4086, Tr_acc: 70.8, val_ac: 74.4
2025-03-08 17:12:12,469 - INFO - Tr_Loss: 1.4444, val_loss: 1.4035, Tr_acc: 72.22857142857143, val_ac: 74.93333333333334
2025-03-08 17:12:12,698 - INFO - Tr_Loss: 1.4351, val_loss: 1.3976, Tr_acc: 72.68571428571428, val_ac: 76.26666666666667
2025-03-08 17:12:12,912 - INFO - Tr_Loss: 1.4314, val_loss: 1.3932, Tr_acc: 73.42857142857143, val_ac: 77.06666666666666
2025-03-08 17:12:13,124 - INFO - Tr_Loss: 1.4298, val_loss: 1.3894, Tr_acc: 75.14285714285714, val_ac: 77.6
2025-03-08 17:12:13,350 - INFO - Tr_Loss: 1.4254, val_loss: 1.3852, Tr_acc: 76.68571428571428, val_ac: 78.26666666666667
2025-03-08 17:12:13,572 - INFO - Tr_Loss: 1.4203, val_loss: 1.3807, Tr_acc: 76.28571428571429, val_ac: 78.93333333333334
2025-03-08 17:12:13,807 - INFO - Tr_Loss: 1.4143, val_loss: 1.3780, Tr_acc: 79.82857142857142, val_ac: 79.73333333333333
2025-03-08 17:12:14,031 - INFO - Tr_Loss: 1.4083, val_loss: 1.3742, Tr_acc: 80.57142857142857, val_ac: 80.26666666666667
2025-03-08 17:12:14,255 - INFO - Tr_Loss: 1.4016, val_loss: 1.3694, Tr_acc: 80.74285714285715, val_ac: 81.06666666666666
2025-03-08 17:12:14,489 - INFO - Tr_Loss: 1.4010, val_loss: 1.3662, Tr_acc: 81.6, val_ac: 81.46666666666667
2025-03-08 17:12:14,717 - INFO - Tr_Loss: 1.3955, val_loss: 1.3643, Tr_acc: 82.68571428571428, val_ac: 81.2
2025-03-08 17:12:14,924 - INFO - Tr_Loss: 1.3916, val_loss: 1.3621, Tr_acc: 83.65714285714286, val_ac: 82.0
2025-03-08 17:12:15,135 - INFO - Tr_Loss: 1.3887, val_loss: 1.3607, Tr_acc: 84.68571428571428, val_ac: 82.4
2025-03-08 17:12:15,352 - INFO - Tr_Loss: 1.3853, val_loss: 1.3592, Tr_acc: 84.62857142857143, val_ac: 82.13333333333334
2025-03-08 17:12:15,560 - INFO - Tr_Loss: 1.3817, val_loss: 1.3565, Tr_acc: 85.65714285714286, val_ac: 82.53333333333333
2025-03-08 17:12:15,786 - INFO - Tr_Loss: 1.3820, val_loss: 1.3553, Tr_acc: 86.11428571428571, val_ac: 83.2
2025-03-08 17:12:16,014 - INFO - Tr_Loss: 1.3769, val_loss: 1.3542, Tr_acc: 87.25714285714285, val_ac: 83.46666666666667
2025-03-08 17:12:16,240 - INFO - Tr_Loss: 1.3748, val_loss: 1.3525, Tr_acc: 87.54285714285714, val_ac: 84.53333333333333
2025-03-08 17:12:16,469 - INFO - Tr_Loss: 1.3704, val_loss: 1.3506, Tr_acc: 87.77142857142857, val_ac: 83.2
2025-03-08 17:12:16,705 - INFO - Tr_Loss: 1.3686, val_loss: 1.3507, Tr_acc: 88.97142857142858, val_ac: 83.86666666666666
2025-03-08 17:12:16,929 - INFO - Tr_Loss: 1.3676, val_loss: 1.3503, Tr_acc: 90.17142857142858, val_ac: 83.6
2025-03-08 17:12:17,166 - INFO - Tr_Loss: 1.3620, val_loss: 1.3483, Tr_acc: 89.54285714285714, val_ac: 83.6
2025-03-08 17:12:17,402 - INFO - Tr_Loss: 1.3605, val_loss: 1.3488, Tr_acc: 90.22857142857143, val_ac: 83.46666666666667
2025-03-08 17:12:17,636 - INFO - Tr_Loss: 1.3602, val_loss: 1.3493, Tr_acc: 90.74285714285715, val_ac: 84.13333333333334
2025-03-08 17:12:17,879 - INFO - Tr_Loss: 1.3563, val_loss: 1.3472, Tr_acc: 91.71428571428571, val_ac: 83.86666666666666
2025-03-08 17:12:18,118 - INFO - Tr_Loss: 1.3535, val_loss: 1.3454, Tr_acc: 91.54285714285714, val_ac: 84.4
2025-03-08 17:12:18,368 - INFO - Tr_Loss: 1.3532, val_loss: 1.3460, Tr_acc: 92.68571428571428, val_ac: 83.73333333333333
2025-03-08 17:12:18,591 - INFO - Tr_Loss: 1.3513, val_loss: 1.3463, Tr_acc: 91.77142857142857, val_ac: 84.53333333333333
2025-03-08 17:12:18,821 - INFO - Tr_Loss: 1.3490, val_loss: 1.3457, Tr_acc: 92.8, val_ac: 84.13333333333334
2025-03-08 17:12:19,045 - INFO - Tr_Loss: 1.3456, val_loss: 1.3437, Tr_acc: 93.2, val_ac: 84.53333333333333
2025-03-08 17:12:19,260 - INFO - Tr_Loss: 1.3418, val_loss: 1.3438, Tr_acc: 93.94285714285714, val_ac: 84.4
2025-03-08 17:12:19,495 - INFO - Tr_Loss: 1.3436, val_loss: 1.3431, Tr_acc: 93.88571428571429, val_ac: 84.93333333333334
2025-03-08 17:12:19,735 - INFO - Tr_Loss: 1.3427, val_loss: 1.3429, Tr_acc: 94.22857142857143, val_ac: 84.53333333333333
2025-03-08 17:12:19,961 - INFO - Tr_Loss: 1.3401, val_loss: 1.3436, Tr_acc: 93.82857142857142, val_ac: 84.53333333333333
2025-03-08 17:12:20,188 - INFO - Tr_Loss: 1.3383, val_loss: 1.3426, Tr_acc: 95.08571428571429, val_ac: 84.4
2025-03-08 17:12:20,401 - INFO - Tr_Loss: 1.3336, val_loss: 1.3428, Tr_acc: 95.77142857142857, val_ac: 84.8
2025-03-08 17:12:20,620 - INFO - Tr_Loss: 1.3317, val_loss: 1.3424, Tr_acc: 95.14285714285714, val_ac: 85.06666666666666
2025-03-08 17:12:20,835 - INFO - Tr_Loss: 1.3319, val_loss: 1.3424, Tr_acc: 95.6, val_ac: 85.2
2025-03-08 17:12:21,045 - INFO - Tr_Loss: 1.3311, val_loss: 1.3425, Tr_acc: 95.42857142857143, val_ac: 84.93333333333334
2025-03-08 17:12:21,253 - INFO - Tr_Loss: 1.3308, val_loss: 1.3430, Tr_acc: 96.17142857142858, val_ac: 84.26666666666667
2025-03-08 17:12:21,503 - INFO - Tr_Loss: 1.3240, val_loss: 1.3409, Tr_acc: 96.45714285714286, val_ac: 84.53333333333333
2025-03-08 17:12:21,739 - INFO - Tr_Loss: 1.3299, val_loss: 1.3412, Tr_acc: 96.28571428571429, val_ac: 85.2
2025-03-08 17:12:22,009 - INFO - Tr_Loss: 1.3238, val_loss: 1.3409, Tr_acc: 96.51428571428572, val_ac: 84.66666666666667
2025-03-08 17:12:22,238 - INFO - Tr_Loss: 1.3219, val_loss: 1.3404, Tr_acc: 97.2, val_ac: 85.06666666666666
2025-03-08 17:12:22,497 - INFO - Tr_Loss: 1.3263, val_loss: 1.3402, Tr_acc: 96.28571428571429, val_ac: 85.2
2025-03-08 17:12:22,732 - INFO - Tr_Loss: 1.3237, val_loss: 1.3419, Tr_acc: 97.2, val_ac: 84.53333333333333
2025-03-08 17:12:22,951 - INFO - Tr_Loss: 1.3214, val_loss: 1.3414, Tr_acc: 97.54285714285714, val_ac: 84.26666666666667
2025-03-08 17:12:23,184 - INFO - Tr_Loss: 1.3212, val_loss: 1.3412, Tr_acc: 97.65714285714286, val_ac: 84.66666666666667
2025-03-08 17:12:23,411 - INFO - Tr_Loss: 1.3142, val_loss: 1.3399, Tr_acc: 97.6, val_ac: 85.2
2025-03-08 17:12:23,636 - INFO - Tr_Loss: 1.3167, val_loss: 1.3406, Tr_acc: 97.48571428571428, val_ac: 85.46666666666667
2025-03-08 17:12:24,077 - INFO - Saving trained model and training results...
2025-03-08 17:12:24,077 - INFO - Starting model evaluation...
2025-03-08 17:12:24,455 - INFO - Test Loss: 1.3347
2025-03-08 17:12:24,455 - INFO - Test Accuracy: 85.60%
2025-03-08 17:12:24,455 - INFO - ======== Model Training Completed! ========
